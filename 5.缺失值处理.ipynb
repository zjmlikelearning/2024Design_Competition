{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 缺失值比例>80%:根据80%法则 (Bijlsma et al. 2006)：当某一物质的非缺失部分低于总样本量的80%时，建议删除该物质.删除缺失值占比超过80%的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hu_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\户数据(预处理中).csv\")\n",
    "ground_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\地块数据(预处理中).csv\")\n",
    "village_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\村数据(预处理中).csv\")\n",
    "\n",
    "# 删除村数据中缺失值占比超过80%的特征列\n",
    "missing_percentage = (village_data.isnull().sum() / len(village_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 80].index\n",
    "village_data = village_data.drop(columns=columns_with_high_missing_percentage)\n",
    "\n",
    "# 删除地块数据中缺失值占比超过80%的特征列\n",
    "missing_percentage = (ground_data.isnull().sum() / len(ground_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 80].index\n",
    "ground_data = ground_data.drop(columns=columns_with_high_missing_percentage)\n",
    "\n",
    "# 删除户数据中缺失值占比超过80%的特征列\n",
    "missing_percentage = (hu_data.isnull().sum() / len(hu_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 80].index\n",
    "hu_data = hu_data.drop(columns=columns_with_high_missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2处理户数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 缺失值比例<80%:采用填充方法,比较均值填充，KNN填充，多重插补，填充后用决策树回归器计算四种模型MAE，选择最优填充模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation - Missing Values Count: 0\n",
      "KNN Imputation - Missing Values Count: 0\n",
      "Iterative Imputation - Missing Values Count: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "# 用租金列不含缺失值的数据训练\n",
    "# 删除 ground_data 中 \"zujin\" 列缺失的样本\n",
    "hu_data_without_missing_zujin = hu_data.dropna(subset=[\"zujin\"])\n",
    "# 用均值填充\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "hu_data_mean = mean_imputer.fit_transform(hu_data_without_missing_zujin)\n",
    "\n",
    "# 用K近邻插补填充\n",
    "knn_imputer = KNNImputer()\n",
    "hu_data_knn = knn_imputer.fit_transform(hu_data_without_missing_zujin)\n",
    "\n",
    "# 用多变量特征插补填充（多重插补）\n",
    "iterative_imputer = IterativeImputer(random_state=42)\n",
    "hu_data_ite = iterative_imputer.fit_transform(hu_data_without_missing_zujin)\n",
    "\n",
    "# 检验是否全部填充完整\n",
    "print(\"Mean Imputation - Missing Values Count:\", np.isnan(hu_data_mean).sum())\n",
    "print(\"KNN Imputation - Missing Values Count:\", np.isnan(hu_data_knn).sum())\n",
    "print(\"Iterative Imputation - Missing Values Count:\", np.isnan(hu_data_ite).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation MAE : 54.298523391812864\n",
      "KNN Imputation MAE : 67.20261695906433\n",
      "Iterative Imputation MAE : 62.372573099415206\n"
     ]
    }
   ],
   "source": [
    "# 用学习后的决策树回归器去预测测试集的标签值，和原始数据集的标签“zujin”对比，计算四种方式的MAE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 定义标签列\n",
    "# 确定 'zujin' 列在第几列\n",
    "zujin_column_index = None\n",
    "for i, column in enumerate(hu_data.columns):\n",
    "    if column == 'zujin':\n",
    "        zujin_column_index = i\n",
    "        break\n",
    "\n",
    "# 提取 'zujin' 列作为标签\n",
    "labels_mean = hu_data_mean[:, zujin_column_index] \n",
    "labels_knn = hu_data_knn[:, zujin_column_index] \n",
    "labels_ite = hu_data_ite[:, zujin_column_index] \n",
    "# 定义决策树回归器\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# 使用五折交叉验证计算 Mean Imputation 的 MAE\n",
    "mae_mean_cv = cross_val_score(tree_regressor, hu_data_mean, labels_mean, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv = -mae_mean_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 使用五折交叉验证计算 KNN Imputation 的 MAE\n",
    "mae_knn_cv = cross_val_score(tree_regressor, hu_data_knn, labels_knn, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_knn_cv = -mae_knn_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 使用五折交叉验证计算 Iterative Imputation 的 MAE\n",
    "mae_ite_cv = cross_val_score(tree_regressor, hu_data_ite, labels_ite, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_ite_cv = -mae_ite_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 输出交叉验证得到的平均绝对误差\n",
    "print(\"Mean Imputation MAE :\", mae_mean_cv)\n",
    "print(\"KNN Imputation MAE :\", mae_knn_cv)\n",
    "print(\"Iterative Imputation MAE :\", mae_ite_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结果显示多数情况下户数据采用均值填充更优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2对填充方法优化：额外处理缺失值比例>50%并且<80%的列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比删除缺失值比例>50%并且<80%的列后用均值填充，删除缺失值比例>80%的列后用均值填充，删除缺失值比例>50%并且<80%的列后线性回归和均值结合填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation80 - Missing Values Count: 0\n",
      "Mean Imputation50 - Missing Values Count: 0\n"
     ]
    }
   ],
   "source": [
    "# 删除缺失值比例>50%的列后用均值填充，删除缺失值比例>50%的列后用均值填充对比\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "hu_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\户数据(预处理中).csv\")\n",
    "# 删除户数据中缺失值占比超过80%的特征列\n",
    "missing_percentage = (hu_data.isnull().sum() / len(hu_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 80].index\n",
    "hu_data80 = hu_data.drop(columns=columns_with_high_missing_percentage)\n",
    "# 删除户数据中缺失值占比超过50%的特征列\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 50].index\n",
    "hu_data50 = hu_data.drop(columns=columns_with_high_missing_percentage)\n",
    "\n",
    "# 用租金列不含缺失值的数据训练\n",
    "# 删除 \"zujin\" 列缺失的样本\n",
    "hu_data_without_missing_zujin80 = hu_data80.dropna(subset=[\"zujin\"])\n",
    "hu_data_without_missing_zujin50 = hu_data50.dropna(subset=[\"zujin\"])\n",
    "# 用均值填充\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "hu_data_mean80 = mean_imputer.fit_transform(hu_data_without_missing_zujin80)\n",
    "hu_data_mean50 = mean_imputer.fit_transform(hu_data_without_missing_zujin50)\n",
    "# 检验是否全部填充完整\n",
    "print(\"Mean Imputation80 - Missing Values Count:\", np.isnan(hu_data_mean80).sum())\n",
    "print(\"Mean Imputation50 - Missing Values Count:\", np.isnan(hu_data_mean50).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation MAE80 : 58.59596491228069\n",
      "Mean Imputation MAE50 : 69.13119883040936\n"
     ]
    }
   ],
   "source": [
    "zujin_column_index80 = None\n",
    "for i, column in enumerate(hu_data80.columns):\n",
    "    if column == 'zujin':\n",
    "        zujin_column_index80 = i\n",
    "        break\n",
    "zujin_column_index50 = None\n",
    "for i, column in enumerate(hu_data50.columns):\n",
    "    if column == 'zujin':\n",
    "        zujin_column_index50 = i\n",
    "        break\n",
    "# 提取 'zujin' 列作为标签\n",
    "labels_mean80 = hu_data_mean80[:, zujin_column_index80] \n",
    "labels_mean50 = hu_data_mean50[:, zujin_column_index50]\n",
    "# 定义决策树回归器\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# 使用五折交叉验证计算 Mean Imputation80 的 MAE\n",
    "mae_mean_cv80 = cross_val_score(tree_regressor, hu_data_mean80, labels_mean80, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv80 = -mae_mean_cv80.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 使用五折交叉验证计算 Mean Imputation50 的 MAE\n",
    "mae_mean_cv50 = cross_val_score(tree_regressor, hu_data_mean50, labels_mean50, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv50 = -mae_mean_cv50.mean()  # 取负号得到正的 MAE\n",
    "print(\"Mean Imputation MAE80 :\", mae_mean_cv80)\n",
    "print(\"Mean Imputation MAE50 :\", mae_mean_cv50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果显示多数情况下删除缺失值比例大于50的特征列效果更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来对比删除缺失值比例>50%的列后用均值填充，线性回归和均值结合填充."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现线性回归和均值结合填充:有单一其他特征与目标特征相关性> 0.5则采用线性回归填充，没有相关性>0.5的特征则用均值填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字段 'hid' 与字段 'year' 的相似性最高，相关性为: 0.10879444549061598\n",
      "字段 'year' 与字段 's_chengbaogd' 的相似性最高，相关性为: 0.2226445111781447\n",
      "字段 'chengbaogd' 与字段 'jycbgdarea' 的相似性最高，相关性为: 0.5577771450189338\n",
      "字段 'chengbaogdks' 与字段 'zcgdksnum' 的相似性最高，相关性为: 0.8109860784317756\n",
      "字段 'jygdarea' 与字段 'jynhgdarea' 的相似性最高，相关性为: 0.9248791762704616\n",
      "字段 'jycbgdarea' 与字段 'chengbaogd' 的相似性最高，相关性为: 0.5577771450189338\n",
      "字段 'jyczgdarea' 与字段 'c_chengbaogd' 的相似性最高，相关性为: 0.1301735693095028\n",
      "字段 'jynhgdarea' 与字段 'jygdarea' 的相似性最高，相关性为: 0.9248791762704616\n",
      "字段 'jygdnum' 与字段 'jygdnum1' 的相似性最高，相关性为: 0.5297853720663166\n",
      "字段 'jygdnum1' 与字段 'jygdnum' 的相似性最高，相关性为: 0.5297853720663166\n",
      "字段 'jygdnum5' 与字段 'jynhgdarea' 的相似性最高，相关性为: 0.5517633067772666\n",
      "字段 'zcgdarea' 与字段 'zcareahetong' 的相似性最高，相关性为: 0.7171671711749427\n",
      "字段 'zcgdksnum' 与字段 'chengbaogdks' 的相似性最高，相关性为: 0.8109860784317756\n",
      "字段 'zcqiyearea' 与字段 'zcgdksnum' 的相似性最高，相关性为: 0.15873951328610372\n",
      "字段 'zchezuoshearea' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.15643115947966557\n",
      "字段 'zccunjitiarea' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.4445453439099428\n",
      "字段 'zcnonghuarea' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.5781205295271297\n",
      "字段 'zcwainonghuarea' 与字段 'zcnonghuarea' 的相似性最高，相关性为: 0.48256568695638247\n",
      "字段 'zcareahetong' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.7171671711749427\n",
      "字段 'zcareaqixian' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.6490576361085623\n",
      "字段 'jitizcarea' 与字段 'jitizcareacgb' 的相似性最高，相关性为: 0.8168062199874281\n",
      "字段 'jitizcareacgb' 与字段 'jitizcarea' 的相似性最高，相关性为: 0.8168062199874281\n",
      "字段 'jitizcareazj' 与字段 'c_chengbaogd' 的相似性最高，相关性为: 0.07172113741167875\n",
      "字段 'zcareashouzu' 与字段 'x_chengbaogd' 的相似性最高，相关性为: 0.08541524565432057\n",
      "字段 'zujin' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.5403413942744355\n",
      "字段 'cbdfangwei' 与字段 'cbdgaosulu' 的相似性最高，相关性为: 0.03472614174429764\n",
      "字段 'cbdarea' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.11684822394954235\n",
      "字段 'cbdcwjuli' 与字段 'cbdwrxiufu' 的相似性最高，相关性为: 0.06833734063902967\n",
      "字段 'cbddljuli' 与字段 'year' 的相似性最高，相关性为: 0.03569097863878458\n",
      "字段 'cbdgaosulu' 与字段 'cbdfangwei' 的相似性最高，相关性为: 0.03472614174429764\n",
      "字段 'cbdguangai' 与字段 'cbdfeili' 的相似性最高，相关性为: 0.13288809794376122\n",
      "字段 'cbdfeili' 与字段 'cbdguangai' 的相似性最高，相关性为: 0.13288809794376122\n",
      "字段 'cbdwrxiufu' 与字段 'year' 的相似性最高，相关性为: 0.13401112006584476\n",
      "字段 'shifouzc' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.1324165879764501\n",
      "字段 'c_chengbaogd' 与字段 'x_chengbaogd' 的相似性最高，相关性为: 0.7681331660217475\n",
      "字段 'x_chengbaogd' 与字段 's_chengbaogd' 的相似性最高，相关性为: 0.9148570193536983\n",
      "字段 's_chengbaogd' 与字段 'x_chengbaogd' 的相似性最高，相关性为: 0.9148570193536983\n",
      "字段 'cychengbaogd' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.6646011595012901\n"
     ]
    }
   ],
   "source": [
    "# 计算相关性\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "hu_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\户数据(预处理中).csv\")\n",
    "# 删除户数据中缺失值占比超过50%的特征列\n",
    "missing_percentage = (hu_data.isnull().sum() / len(hu_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 50].index\n",
    "hu_data50 = hu_data.drop(columns=columns_with_high_missing_percentage)\n",
    "hu_data50 = hu_data50.dropna(subset=[\"zujin\"])\n",
    "# 复制数据集\n",
    "Corr_data = hu_data50.copy()\n",
    "# 各字段间相关程度\n",
    "correlation = Corr_data.corr()\n",
    "# 创建一个空字典来保存每个字段与其相似性最高的另一字段名称及相似性\n",
    "similarities = {}\n",
    "# 遍历数据框的列\n",
    "for column in Corr_data.columns:\n",
    "    # 计算该列与其他列的相关性\n",
    "    correlations = Corr_data.corr()[column].drop(column)\n",
    "    # 找到与当前列相关性最高的字段名和相似性值\n",
    "    most_similar_column = correlations.idxmax()\n",
    "    highest_correlation = correlations.max()\n",
    "    # 保存到字典中\n",
    "    similarities[column] = (most_similar_column, highest_correlation)\n",
    "# 输出结果\n",
    "for column, (most_similar_column, highest_correlation) in similarities.items():\n",
    "    print(f\"字段 '{column}' 与字段 '{most_similar_column}' 的相似性最高，相关性为: {highest_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列'hid'均值填充\n",
      "列'chengbaogdks'均值填充\n",
      "列'jygdarea'均值填充\n",
      "列'jycbgdarea'线性回归预测填充\n",
      "列'jyczgdarea'均值填充\n",
      "列'jynhgdarea'线性回归预测填充\n",
      "列'jygdnum'均值填充\n",
      "列'jygdnum1'线性回归预测填充\n",
      "列'jygdnum5'线性回归预测填充\n",
      "列'zcgdarea'均值填充\n",
      "列'zcgdksnum'线性回归预测填充\n",
      "列'zcqiyearea'均值填充\n",
      "列'zchezuoshearea'均值填充\n",
      "列'zccunjitiarea'均值填充\n",
      "列'zcnonghuarea'线性回归预测填充\n",
      "列'zcwainonghuarea'均值填充\n",
      "列'zcareahetong'线性回归预测填充\n",
      "列'zcareaqixian'线性回归预测填充\n",
      "列'jitizcarea'均值填充\n",
      "列'jitizcareacgb'线性回归预测填充\n",
      "列'jitizcareazj'均值填充\n",
      "列'zcareashouzu'均值填充\n",
      "列'cbdfangwei'均值填充\n",
      "列'cbdarea'均值填充\n",
      "列'cbdcwjuli'均值填充\n",
      "列'cbddljuli'均值填充\n",
      "列'cbdgaosulu'均值填充\n",
      "列'cbdguangai'均值填充\n",
      "列'cbdfeili'均值填充\n",
      "列'cbdwrxiufu'均值填充\n",
      "列'shifouzc'均值填充\n",
      "数据中不存在任何缺失值。\n"
     ]
    }
   ],
   "source": [
    "# 定义填充函数\n",
    "def fill_missing_values(df, column, similar_column, data):\n",
    "    if df[column].isnull().any():  # 检查目标列是否有缺失值\n",
    "        correlation = data\n",
    "\n",
    "        if correlation < 0.5 or df[similar_column].isnull().any():\n",
    "            # 使用均值填充\n",
    "            print(f\"列'{column}'均值填充\")\n",
    "            mean_value = df[column].mean()\n",
    "            df[column].fillna(mean_value, inplace=True)\n",
    "        else:\n",
    "            # 使用线性回归预测填充\n",
    "            print(f\"列'{column}'线性回归预测填充\")\n",
    "            known_data = df[[column, similar_column]].dropna()\n",
    "            unknown_data = df[df[column].isnull()]\n",
    "            \n",
    "            # 定义特征和目标列\n",
    "            features = [similar_column]\n",
    "            target_columns = [column]\n",
    "\n",
    "            # 划分训练集和测试集\n",
    "            X_train  = known_data[features] \n",
    "            y_train=known_data[target_columns]\n",
    "            # 训练线性回归模型\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # 预测缺失值\n",
    "            predicted_values = model.predict(unknown_data[features])\n",
    "\n",
    "            # 填充缺失值\n",
    "            unknown_data[target_columns] = predicted_values\n",
    "            filled_data = known_data._append(unknown_data)\n",
    "            \n",
    "            df.loc[filled_data.index, [column, similar_column]] = filled_data[[column, similar_column]]\n",
    "\n",
    "# 遍历每个字段\n",
    "for key, (first_element, second_element) in similarities.items():\n",
    "    fill_missing_values(Corr_data, key, first_element, second_element)\n",
    "# 检查是否还有缺失值\n",
    "missing_values = Corr_data.isnull().sum().sum()\n",
    "if missing_values == 0:\n",
    "    print(\"数据中不存在任何缺失值。\")\n",
    "else:\n",
    "    print(f\"数据中仍有缺失值。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值和线性回归结合填充 : 67.6263596491228\n",
      "均值填充 : 53.07527777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 提取 'zujin' 列作为标签\n",
    "labels = hu_data50[\"zujin\"]\n",
    "# 定义决策树回归器\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "mae_mean_cv_line = cross_val_score(tree_regressor, Corr_data, labels, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv_line = -mae_mean_cv_line.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "mae_mean_cv50 = cross_val_score(tree_regressor, hu_data_mean50, labels, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv50 = -mae_mean_cv50.mean()  # 取负号得到正的 MAE\n",
    "print(\"均值和线性回归结合填充 :\", mae_mean_cv_line)\n",
    "print(\"均值填充 :\", mae_mean_cv50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3最终填充：结果显示采用线性回归和均值填充结合的方法效果更好，如果线性回归预测填充出现了负数，那对这一列改成采用均值填充，分类数据采用众数填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列'hid'均值填充\n",
      "列'chengbaogd'均值填充\n",
      "列'jygdarea'均值填充\n",
      "列'jycbgdarea'线性回归预测填充\n",
      "列'jyczgdarea'均值填充\n",
      "列'jynhgdarea'线性回归预测填充\n",
      "列'zcgdarea'均值填充\n",
      "列'zcqiyearea'均值填充\n",
      "列'zchezuoshearea'均值填充\n",
      "列'zccunjitiarea'线性回归预测填充\n",
      "列'zcnonghuarea'线性回归预测填充\n",
      "列'zcwainonghuarea'均值填充\n",
      "列'zcareahetong'线性回归预测填充\n",
      "列'zcareaqixian'线性回归预测填充\n",
      "列'jitizcarea'均值填充\n",
      "列'jitizcareacgb'线性回归预测填充\n",
      "列'jitizcareazj'均值填充\n",
      "列'zcareashouzu'均值填充\n",
      "列'zujin'线性回归预测填充\n",
      "列'cbdarea'均值填充\n",
      "列'cbdcwjuli'均值填充\n",
      "列'cbddljuli'均值填充\n",
      "列'cychengbaogd'均值填充\n",
      "存在负值，将使用填充前的数据的平均值来替换。\n",
      "数据中不存在任何缺失值。\n"
     ]
    }
   ],
   "source": [
    "# 用线性回归结合均值填充\n",
    "# 删除户数据中缺失值占比超过50%的特征列\n",
    "missing_percentage = (hu_data.isnull().sum() / len(hu_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 50].index\n",
    "hu_data50 = hu_data.drop(columns=columns_with_high_missing_percentage)\n",
    "# 对分类数据缺失值采用众数填充\n",
    "columns_to_fill = [\"cbdfangwei\",\n",
    "                    \"cbdpodu\", \"zrdpodu\", \"cbdgaosulu\", \n",
    "                    \"zrdgaosulu\", \"cbdturang\", \"zrdturang\", \"cbdguangai\", \n",
    "                    \"zrdguangai\", \"cbdfeili\", \"zrdfeili\", \"cbdyongtu201912\",\n",
    "                    \"zrdyongtu201912\", \"cbdyongtu202008\", \"zrdyongtu202008\", \n",
    "                    \"cbdwrxiufu\", \"zrdwrxiufu\", \"shifouzc\", \"cbdjinqin\", \n",
    "                    \"zrdjinqin\", \"cbdbutie\", \"zrdbutie\", \"cbdxietiao\", \n",
    "                    \"zrdxietiao\", \"cbdhetong\", \"zrdhetong\", \"cbdjypt\",\n",
    "                    \"zrdjypt\", \"cbdjyptreason1\", \"zrdjyptreason1\",\n",
    "                    \"cbdjyptreason2\", \"zrdjyptreason2\", \"cbdjiangqixian\", \n",
    "                    \"zrdjiangqixian\", \"d301a\", \"d301b\", \"d311a\", \"d311b\", \n",
    "                    \"d312a\", \"d312b\", \"d313a\", \"d313b\", \"d314a\", \"d314b\", \n",
    "                    \"d315a\", \"d315b\", \"d316a\", \"d316b\", \"d317a\", \"d317b\", \n",
    "                    \"d319a\", \"d319b\", \"d320a\", \"d320b\", \"d321a\", \"d321b\", \n",
    "                    \"d322a\", \"d322b\", \"d323a\", \"d323b\", \"d324a\", \"d324b\", \n",
    "                    \"d325a\", \"d325b\", \"d328a\", \"d328b\", \"d329a\", \"d329b\", \n",
    "                    \"d331a\", \"d331b\", \"d335a\", \"d335b\", \"d336a\", \"d336b\", \n",
    "                    \"d337a\", \"d337b\", \"d338a\", \"d338b\", \"d339a\", \"d339b\", \n",
    "                    \"d340a\", \"d340b\", \"d341a\", \"d341b\",\"jygdnum\",\"jygdnum1\",\"jygdnum5\",\n",
    "                    \"zcgdksnum\",\"zcgdksnum\",\"chengbaogdks\"]\n",
    "\n",
    "# 填充缺失值\n",
    "for column in hu_data50:\n",
    "    if column in columns_to_fill:\n",
    "        mode_val = hu_data50[column].mode()[0]  # 计算众数并取第一个值\n",
    "        hu_data50[column].fillna(mode_val, inplace=True)\n",
    "# 复制数据集\n",
    "Corr_data = hu_data50.copy()\n",
    "# 各字段间相关程度\n",
    "correlation = Corr_data.corr()\n",
    "# 创建一个空字典来保存每个字段与其相似性最高的另一字段名称及相似性\n",
    "similarities = {}\n",
    "# 遍历数据框的列\n",
    "for column in Corr_data.columns:\n",
    "    # 计算该列与其他列的相关性\n",
    "    correlations = Corr_data.corr()[column].drop(column)\n",
    "    # 找到与当前列相关性最高的字段名和相似性值\n",
    "    most_similar_column = correlations.idxmax()\n",
    "    highest_correlation = correlations.max()\n",
    "    # 保存到字典中\n",
    "    similarities[column] = (most_similar_column, highest_correlation)\n",
    "# 定义填充函数\n",
    "def fill_missing_values(df, column, similar_column, data):\n",
    "    if df[column].isnull().any():  # 检查目标列是否有缺失值\n",
    "        correlation = data\n",
    "\n",
    "        if correlation < 0.5 or df[similar_column].isnull().any():\n",
    "            # 使用均值填充\n",
    "            print(f\"列'{column}'均值填充\")\n",
    "            mean_value = df[column].mean()\n",
    "            df[column].fillna(mean_value, inplace=True)\n",
    "        else:\n",
    "            # 使用线性回归预测填充\n",
    "            print(f\"列'{column}'线性回归预测填充\")\n",
    "            known_data = df[[column, similar_column]].dropna()\n",
    "            unknown_data = df[df[column].isnull()]\n",
    "            \n",
    "            # 定义特征和目标列\n",
    "            features = [similar_column]\n",
    "            target_columns = [column]\n",
    "\n",
    "            # 划分训练集和测试集\n",
    "            X_train  = known_data[features] \n",
    "            y_train=known_data[target_columns]\n",
    "            # 训练线性回归模型\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # 预测缺失值\n",
    "            predicted_values = model.predict(unknown_data[features])\n",
    "\n",
    "            # 填充缺失值\n",
    "            unknown_data[target_columns] = predicted_values\n",
    "            filled_data = known_data._append(unknown_data)\n",
    "            \n",
    "            df.loc[filled_data.index, [column, similar_column]] = filled_data[[column, similar_column]]\n",
    "\n",
    "# 遍历每个字段\n",
    "for key, (first_element, second_element) in similarities.items():\n",
    "    fill_missing_values(Corr_data, key, first_element, second_element)\n",
    "\n",
    "# 填充缺失值后检查是否存在负值\n",
    "negative_values = (Corr_data < 0).any().any()\n",
    "if negative_values:\n",
    "    print(\"存在负值，将使用填充前的数据的平均值来替换。\")\n",
    "    # 填充前的数据\n",
    "    original_data = hu_data50\n",
    "\n",
    "    for column in Corr_data.columns:\n",
    "        # 只对存在负值的列进行处理\n",
    "        negative_mask = Corr_data[column] < 0\n",
    "        if negative_mask.any():\n",
    "            # 使用填充前的数据的平均值替换负值\n",
    "            mean_value = original_data[column].mean()\n",
    "            Corr_data.loc[negative_mask, column] = mean_value\n",
    "# 检查是否还有缺失值\n",
    "missing_values = Corr_data.isnull().sum().sum()\n",
    "if missing_values == 0:\n",
    "    print(\"数据中不存在任何缺失值。\")\n",
    "    hu_data=Corr_data\n",
    "else:\n",
    "    print(f\"数据中仍有缺失值。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_data.to_csv(r\"C:\\Users\\12045\\Desktop\\户数据(无标准化用于可视化).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 提取除了指定列之外的其他列\n",
    "columns_to_normalize = [col for col in hu_data.columns if col not in[\"cbdfangwei\",\n",
    "                    \"cbdpodu\", \"zrdpodu\", \"cbdgaosulu\", \n",
    "                    \"zrdgaosulu\", \"cbdturang\", \"zrdturang\", \"cbdguangai\", \n",
    "                    \"zrdguangai\", \"cbdfeili\", \"zrdfeili\", \"cbdyongtu201912\",\n",
    "                    \"zrdyongtu201912\", \"cbdyongtu202008\", \"zrdyongtu202008\", \n",
    "                    \"cbdwrxiufu\", \"zrdwrxiufu\", \"shifouzc\", \"cbdjinqin\", \n",
    "                    \"zrdjinqin\", \"cbdbutie\", \"zrdbutie\", \"cbdxietiao\", \n",
    "                    \"zrdxietiao\", \"cbdhetong\", \"zrdhetong\", \"cbdjypt\",\n",
    "                    \"zrdjypt\", \"cbdjyptreason1\", \"zrdjyptreason1\",\n",
    "                    \"cbdjyptreason2\", \"zrdjyptreason2\", \"cbdjiangqixian\", \n",
    "                    \"zrdjiangqixian\", \"d301a\", \"d301b\", \"d311a\", \"d311b\", \n",
    "                    \"d312a\", \"d312b\", \"d313a\", \"d313b\", \"d314a\", \"d314b\", \n",
    "                    \"d315a\", \"d315b\", \"d316a\", \"d316b\", \"d317a\", \"d317b\", \n",
    "                    \"d319a\", \"d319b\", \"d320a\", \"d320b\", \"d321a\", \"d321b\", \n",
    "                    \"d322a\", \"d322b\", \"d323a\", \"d323b\", \"d324a\", \"d324b\", \n",
    "                    \"d325a\", \"d325b\", \"d328a\", \"d328b\", \"d329a\", \"d329b\", \n",
    "                    \"d331a\", \"d331b\", \"d335a\", \"d335b\", \"d336a\", \"d336b\", \n",
    "                    \"d337a\", \"d337b\", \"d338a\", \"d338b\", \"d339a\", \"d339b\", \n",
    "                    \"d340a\", \"d340b\", \"d341a\", \"d341b\",\"jygdnum\",\"jygdnum1\",\"jygdnum5\",\n",
    "                    \"zcgdksnum\",\"zcgdksnum\",\"chengbaogdks\"]]\n",
    "\n",
    "# 对这些列进行标准化处理\n",
    "scaler = StandardScaler()\n",
    "hu_data[columns_to_normalize] = scaler.fit_transform(hu_data[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu_data.to_csv(r\"C:\\Users\\12045\\Desktop\\户数据(预处理后).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3处理地块数据，方法同用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation - Missing Values Count: 0\n",
      "KNN Imputation - Missing Values Count: 0\n",
      "Iterative Imputation - Missing Values Count: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "ground_data_without_missing_zujin = ground_data.dropna(subset=[\"zujin\"])\n",
    "# 用均值填充\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "ground_data_mean = mean_imputer.fit_transform(ground_data_without_missing_zujin)\n",
    "\n",
    "# 用K近邻插补填充\n",
    "knn_imputer = KNNImputer()\n",
    "ground_data_knn = knn_imputer.fit_transform(ground_data_without_missing_zujin)\n",
    "\n",
    "# 用多变量特征插补填充（多重插补）\n",
    "iterative_imputer = IterativeImputer(random_state=42)\n",
    "ground_data_ite = iterative_imputer.fit_transform(ground_data_without_missing_zujin)\n",
    "\n",
    "# 检验是否全部填充完整\n",
    "print(\"Mean Imputation - Missing Values Count:\", np.isnan(ground_data_mean).sum())\n",
    "print(\"KNN Imputation - Missing Values Count:\", np.isnan(ground_data_knn).sum())\n",
    "print(\"Iterative Imputation - Missing Values Count:\", np.isnan(ground_data_ite).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation MAE : 472.8026890756302\n",
      "KNN Imputation MAE : 449.42626050420165\n",
      "Iterative Imputation MAE : 470.17546218487394\n"
     ]
    }
   ],
   "source": [
    "# 用学习后的决策树回归器去预测测试集的标签值，和原始数据集的标签“zujin”对比，计算四种方式的MAE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 定义标签列\n",
    "# 确定 'zujin' 列在第几列\n",
    "zujin_column_index = None\n",
    "for i, column in enumerate(ground_data.columns):\n",
    "    if column == 'zujin':\n",
    "        zujin_column_index = i\n",
    "        break\n",
    "\n",
    "# 提取 'zujin' 列作为标签\n",
    "labels_mean = ground_data_mean[:, zujin_column_index] \n",
    "labels_knn = ground_data_knn[:, zujin_column_index] \n",
    "labels_ite = ground_data_ite[:, zujin_column_index] \n",
    "# 定义决策树回归器\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "# 使用五折交叉验证计算 Mean Imputation 的 MAE\n",
    "mae_mean_cv = cross_val_score(tree_regressor, ground_data_mean, labels_mean, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv = -mae_mean_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 使用五折交叉验证计算 KNN Imputation 的 MAE\n",
    "mae_knn_cv = cross_val_score(tree_regressor,ground_data_knn, labels_knn, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_knn_cv = -mae_knn_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 使用五折交叉验证计算 Iterative Imputation 的 MAE\n",
    "mae_ite_cv = cross_val_score(tree_regressor, ground_data_ite, labels_ite, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_ite_cv = -mae_ite_cv.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "# 输出交叉验证得到的平均绝对误差\n",
    "print(\"Mean Imputation MAE :\", mae_mean_cv)\n",
    "print(\"KNN Imputation MAE :\", mae_knn_cv)\n",
    "print(\"Iterative Imputation MAE :\", mae_ite_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果显示地块数据采用均值填充更优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字段 'hid' 与字段 'zrdjyptreason2' 的相似性最高，相关性为: 0.11961624075250007\n",
      "字段 'year' 与字段 'd329a' 的相似性最高，相关性为: 0.4787478874903556\n",
      "字段 'chengbaogd' 与字段 'd330b' 的相似性最高，相关性为: 0.7013004589518397\n",
      "字段 'chengbaogdks' 与字段 'cychengbaogd' 的相似性最高，相关性为: 0.522779895992503\n",
      "字段 'jygdarea' 与字段 'jynhgdarea' 的相似性最高，相关性为: 0.868035960230705\n",
      "字段 'jycbgdarea' 与字段 'd330b' 的相似性最高，相关性为: 0.7271250440782292\n",
      "字段 'jyczgdarea' 与字段 'zcareaqixian' 的相似性最高，相关性为: 0.3299121935967518\n",
      "字段 'jynhgdarea' 与字段 'jygdarea' 的相似性最高，相关性为: 0.868035960230705\n",
      "字段 'jygdnum' 与字段 'jygdnum1' 的相似性最高，相关性为: 0.6318008893383532\n",
      "字段 'jygdnum1' 与字段 'jygdnum' 的相似性最高，相关性为: 0.6318008893383532\n",
      "字段 'jygdnum5' 与字段 'jynhgdarea' 的相似性最高，相关性为: 0.5272272154981769\n",
      "字段 'zcgdarea' 与字段 'jitizcarea' 的相似性最高，相关性为: 0.8244847532316224\n",
      "字段 'zcqiyearea' 与字段 'd323a' 的相似性最高，相关性为: 0.3586414850416256\n",
      "字段 'zchezuoshearea' 与字段 'zcareahetong' 的相似性最高，相关性为: 0.2502324241919913\n",
      "字段 'zccunjitiarea' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.5484679015956245\n",
      "字段 'zcnonghuarea' 与字段 'd315a' 的相似性最高，相关性为: 0.5169247759454143\n",
      "字段 'zcareahetong' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.6851825174031491\n",
      "字段 'zcareaqixian' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.4899272734444373\n",
      "字段 'jitizcarea' 与字段 'jitizcareacgb' 的相似性最高，相关性为: 0.8994499857353342\n",
      "字段 'jitizcareacgb' 与字段 'jitizcarea' 的相似性最高，相关性为: 0.8994499857353342\n",
      "字段 'zujin' 与字段 'd315a' 的相似性最高，相关性为: 0.5683934342852143\n",
      "字段 'cbdfangwei' 与字段 'zrdfangwei' 的相似性最高，相关性为: 0.5950019924355886\n",
      "字段 'zrdfangwei' 与字段 'cbdfangwei' 的相似性最高，相关性为: 0.5950019924355886\n",
      "字段 'cbdarea' 与字段 'cychengbaogd' 的相似性最高，相关性为: 0.5331390038053553\n",
      "字段 'zrdarea' 与字段 'd318b' 的相似性最高，相关性为: 0.7380074008630706\n",
      "字段 'cbdcwjuli' 与字段 'd327a' 的相似性最高，相关性为: 0.3242244498246487\n",
      "字段 'zrdcwjuli' 与字段 'd327a' 的相似性最高，相关性为: 0.3445472341974818\n",
      "字段 'cbddljuli' 与字段 'zrddljuli' 的相似性最高，相关性为: 0.5999367758340516\n",
      "字段 'zrddljuli' 与字段 'cbddljuli' 的相似性最高，相关性为: 0.5999367758340516\n",
      "字段 'cbdgaosulu' 与字段 'zrdgaosulu' 的相似性最高，相关性为: 0.6604527108704198\n",
      "字段 'zrdgaosulu' 与字段 'cbdgaosulu' 的相似性最高，相关性为: 0.6604527108704198\n",
      "字段 'cbdguangai' 与字段 'zrdguangai' 的相似性最高，相关性为: 0.6713710987358749\n",
      "字段 'zrdguangai' 与字段 'cbdguangai' 的相似性最高，相关性为: 0.6713710987358749\n",
      "字段 'cbdfeili' 与字段 'zrdfeili' 的相似性最高，相关性为: 0.7513732523112289\n",
      "字段 'zrdfeili' 与字段 'cbdfeili' 的相似性最高，相关性为: 0.7513732523112289\n",
      "字段 'cbdwrxiufu' 与字段 'zrdwrxiufu' 的相似性最高，相关性为: 0.736137779221664\n",
      "字段 'zrdwrxiufu' 与字段 'cbdwrxiufu' 的相似性最高，相关性为: 0.736137779221664\n",
      "字段 'shifouzc' 与字段 'd315a' 的相似性最高，相关性为: 0.3187393083959021\n",
      "字段 'zrdyear' 与字段 'zrdsyqixian' 的相似性最高，相关性为: 0.42820849546150386\n",
      "字段 'zrdsyqixian' 与字段 'zrdyear' 的相似性最高，相关性为: 0.42820849546150386\n",
      "字段 'zrdjinqin' 与字段 'zrdjyptreason2' 的相似性最高，相关性为: 0.6482409695326973\n",
      "字段 'zrdxietiao' 与字段 'd332a' 的相似性最高，相关性为: 0.43163691207102484\n",
      "字段 'zrdhetong' 与字段 'd324a' 的相似性最高，相关性为: 0.4779147093393633\n",
      "字段 'zrdjyptreason1' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.4928618380421372\n",
      "字段 'zrdjyptreason2' 与字段 'zrdjinqin' 的相似性最高，相关性为: 0.6482409695326973\n",
      "字段 'zrdzuyue' 与字段 'c_chengbaogd' 的相似性最高，相关性为: 0.36834094208752927\n",
      "字段 'd302a' 与字段 'd302b' 的相似性最高，相关性为: 0.6515432794108624\n",
      "字段 'd302b' 与字段 'd302a' 的相似性最高，相关性为: 0.6515432794108624\n",
      "字段 'd303a' 与字段 'd303b' 的相似性最高，相关性为: 0.9076807722195025\n",
      "字段 'd303b' 与字段 'd303a' 的相似性最高，相关性为: 0.9076807722195025\n",
      "字段 'd306a' 与字段 'd306b' 的相似性最高，相关性为: 0.7854022847425486\n",
      "字段 'd306b' 与字段 'd306a' 的相似性最高，相关性为: 0.7854022847425486\n",
      "字段 'd307a' 与字段 'd307b' 的相似性最高，相关性为: 0.7901297702611286\n",
      "字段 'd307b' 与字段 'd307a' 的相似性最高，相关性为: 0.7901297702611286\n",
      "字段 'd308a' 与字段 'd308b' 的相似性最高，相关性为: 0.9720188631872362\n",
      "字段 'd308b' 与字段 'd308a' 的相似性最高，相关性为: 0.9720188631872362\n",
      "字段 'd309b' 与字段 'd308a' 的相似性最高，相关性为: 0.6923344267745616\n",
      "字段 'd312a' 与字段 'd312b' 的相似性最高，相关性为: 0.8706166425952534\n",
      "字段 'd312b' 与字段 'd312a' 的相似性最高，相关性为: 0.8706166425952534\n",
      "字段 'd313a' 与字段 'd315a' 的相似性最高，相关性为: 0.5817306085852172\n",
      "字段 'd314a' 与字段 'd314b' 的相似性最高，相关性为: 0.8624575450871765\n",
      "字段 'd314b' 与字段 'd314a' 的相似性最高，相关性为: 0.8624575450871765\n",
      "字段 'd315a' 与字段 'd315b' 的相似性最高，相关性为: 0.8050112948805689\n",
      "字段 'd315b' 与字段 'd315a' 的相似性最高，相关性为: 0.8050112948805689\n",
      "字段 'd316a' 与字段 'd316b' 的相似性最高，相关性为: 0.831004548116081\n",
      "字段 'd316b' 与字段 'd316a' 的相似性最高，相关性为: 0.831004548116081\n",
      "字段 'd317a' 与字段 'd317b' 的相似性最高，相关性为: 0.8706166425952532\n",
      "字段 'd317b' 与字段 'd317a' 的相似性最高，相关性为: 0.8706166425952532\n",
      "字段 'd318a' 与字段 'd332b' 的相似性最高，相关性为: 0.6510493683812687\n",
      "字段 'd318b' 与字段 'zrdarea' 的相似性最高，相关性为: 0.7380074008630706\n",
      "字段 'd320a' 与字段 'd322b' 的相似性最高，相关性为: 0.5801193511153213\n",
      "字段 'd322a' 与字段 'd322b' 的相似性最高，相关性为: 0.9233439784631197\n",
      "字段 'd322b' 与字段 'd322a' 的相似性最高，相关性为: 0.9233439784631197\n",
      "字段 'd323a' 与字段 'd323b' 的相似性最高，相关性为: 0.946534522249429\n",
      "字段 'd323b' 与字段 'd323a' 的相似性最高，相关性为: 0.946534522249429\n",
      "字段 'd324a' 与字段 'd324b' 的相似性最高，相关性为: 0.7261904761904762\n",
      "字段 'd324b' 与字段 'd324a' 的相似性最高，相关性为: 0.7261904761904762\n",
      "字段 'd325a' 与字段 'd322a' 的相似性最高，相关性为: 0.7400419287211739\n",
      "字段 'd327a' 与字段 'd327b' 的相似性最高，相关性为: 0.6224645002968038\n",
      "字段 'd327b' 与字段 'd327a' 的相似性最高，相关性为: 0.6224645002968038\n",
      "字段 'd328a' 与字段 'd328b' 的相似性最高，相关性为: 0.9378408471299673\n",
      "字段 'd328b' 与字段 'd328a' 的相似性最高，相关性为: 0.9378408471299673\n",
      "字段 'd329a' 与字段 'd329b' 的相似性最高，相关性为: 0.9593230696067404\n",
      "字段 'd329b' 与字段 'd329a' 的相似性最高，相关性为: 0.9593230696067404\n",
      "字段 'd330a' 与字段 'd327b' 的相似性最高，相关性为: 0.3151727657908484\n",
      "字段 'd330b' 与字段 'jycbgdarea' 的相似性最高，相关性为: 0.7271250440782292\n",
      "字段 'd332a' 与字段 'd332b' 的相似性最高，相关性为: 0.6030285461373671\n",
      "字段 'd332b' 与字段 'd318a' 的相似性最高，相关性为: 0.6510493683812687\n",
      "字段 'd335a' 与字段 'd335b' 的相似性最高，相关性为: 0.7221549421918749\n",
      "字段 'd335b' 与字段 'd335a' 的相似性最高，相关性为: 0.7221549421918749\n",
      "字段 'c_chengbaogd' 与字段 'x_chengbaogd' 的相似性最高，相关性为: 0.8905499253561879\n",
      "字段 'x_chengbaogd' 与字段 's_chengbaogd' 的相似性最高，相关性为: 0.9120097877730201\n",
      "字段 's_chengbaogd' 与字段 'x_chengbaogd' 的相似性最高，相关性为: 0.9120097877730201\n",
      "字段 'cychengbaogd' 与字段 'zcgdarea' 的相似性最高，相关性为: 0.8239360256349224\n"
     ]
    }
   ],
   "source": [
    "# 计算相关性\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "ground_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\地块数据(预处理中).csv\")\n",
    "# 删除户数据中缺失值占比超过50%的特征列，但保留 \"zujin\" 列\n",
    "missing_percentage = (ground_data.isnull().sum() / len(ground_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 50].index\n",
    "columns_with_high_missing_percentage = columns_with_high_missing_percentage.drop(\"zujin\")\n",
    "ground_data50 = ground_data.drop(columns=columns_with_high_missing_percentage)\n",
    "# 删除 \"zujin\" 列中的缺失值\n",
    "ground_data50 = ground_data50.dropna(subset=[\"zujin\"])\n",
    "# 复制数据集\n",
    "Corr_data = ground_data50.copy()\n",
    "# 各字段间相关程度\n",
    "correlation = Corr_data.corr()\n",
    "# 创建一个空字典来保存每个字段与其相似性最高的另一字段名称及相似性\n",
    "similarities = {}\n",
    "# 遍历数据框的列\n",
    "for column in Corr_data.columns:\n",
    "    # 计算该列与其他列的相关性\n",
    "    correlations = Corr_data.corr()[column].drop(column)\n",
    "    # 找到与当前列相关性最高的字段名和相似性值\n",
    "    most_similar_column = correlations.idxmax()\n",
    "    highest_correlation = correlations.max()\n",
    "    # 保存到字典中\n",
    "    similarities[column] = (most_similar_column, highest_correlation)\n",
    "# 输出结果\n",
    "for column, (most_similar_column, highest_correlation) in similarities.items():\n",
    "    print(f\"字段 '{column}' 与字段 '{most_similar_column}' 的相似性最高，相关性为: {highest_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列'chengbaogd'均值填充\n",
      "列'chengbaogdks'线性回归预测填充\n",
      "列'jygdarea'均值填充\n",
      "列'jycbgdarea'均值填充\n",
      "列'jyczgdarea'均值填充\n",
      "列'jynhgdarea'线性回归预测填充\n",
      "列'jygdnum'均值填充\n",
      "列'jygdnum1'线性回归预测填充\n",
      "列'jygdnum5'线性回归预测填充\n",
      "列'zcgdarea'均值填充\n",
      "列'zcqiyearea'均值填充\n",
      "列'zchezuoshearea'均值填充\n",
      "列'zccunjitiarea'线性回归预测填充\n",
      "列'zcnonghuarea'均值填充\n",
      "列'zcareahetong'线性回归预测填充\n",
      "列'zcareaqixian'均值填充\n",
      "列'jitizcarea'均值填充\n",
      "列'jitizcareacgb'线性回归预测填充\n",
      "列'cbdfangwei'均值填充\n",
      "列'zrdfangwei'线性回归预测填充\n",
      "列'cbdarea'线性回归预测填充\n",
      "列'cbdcwjuli'均值填充\n",
      "列'zrdcwjuli'均值填充\n",
      "列'cbddljuli'均值填充\n",
      "列'zrddljuli'线性回归预测填充\n",
      "列'cbdgaosulu'均值填充\n",
      "列'zrdgaosulu'线性回归预测填充\n",
      "列'cbdguangai'均值填充\n",
      "列'zrdguangai'线性回归预测填充\n",
      "列'cbdfeili'均值填充\n",
      "列'zrdfeili'线性回归预测填充\n",
      "列'cbdwrxiufu'均值填充\n",
      "列'zrdwrxiufu'线性回归预测填充\n",
      "列'shifouzc'均值填充\n",
      "列'zrdyear'均值填充\n",
      "列'zrdsyqixian'均值填充\n",
      "列'zrdjinqin'均值填充\n",
      "列'zrdxietiao'均值填充\n",
      "列'zrdhetong'均值填充\n",
      "列'zrdjyptreason1'均值填充\n",
      "列'zrdjyptreason2'线性回归预测填充\n",
      "列'zrdzuyue'均值填充\n",
      "列'd302a'均值填充\n",
      "列'd302b'线性回归预测填充\n",
      "列'd303a'均值填充\n",
      "列'd303b'线性回归预测填充\n",
      "列'd306a'均值填充\n",
      "列'd306b'线性回归预测填充\n",
      "列'd307a'均值填充\n",
      "列'd307b'线性回归预测填充\n",
      "列'd308a'均值填充\n",
      "列'd308b'线性回归预测填充\n",
      "列'd309b'线性回归预测填充\n",
      "列'd312a'均值填充\n",
      "列'd312b'线性回归预测填充\n",
      "列'd313a'均值填充\n",
      "列'd314a'均值填充\n",
      "列'd314b'线性回归预测填充\n",
      "列'd315a'均值填充\n",
      "列'd315b'线性回归预测填充\n",
      "列'd316a'均值填充\n",
      "列'd316b'线性回归预测填充\n",
      "列'd317a'均值填充\n",
      "列'd317b'线性回归预测填充\n",
      "列'd318a'均值填充\n",
      "列'd318b'线性回归预测填充\n",
      "列'd320a'均值填充\n",
      "列'd322a'均值填充\n",
      "列'd322b'线性回归预测填充\n",
      "列'd323a'均值填充\n",
      "列'd323b'线性回归预测填充\n",
      "列'd324a'均值填充\n",
      "列'd324b'线性回归预测填充\n",
      "列'd325a'线性回归预测填充\n",
      "列'd327a'均值填充\n",
      "列'd327b'线性回归预测填充\n",
      "列'd328a'均值填充\n",
      "列'd328b'线性回归预测填充\n",
      "列'd329a'均值填充\n",
      "列'd329b'线性回归预测填充\n",
      "列'd330a'均值填充\n",
      "列'd330b'线性回归预测填充\n",
      "列'd332a'均值填充\n",
      "列'd332b'线性回归预测填充\n",
      "列'd335a'均值填充\n",
      "列'd335b'线性回归预测填充\n",
      "数据中不存在任何缺失值。\n"
     ]
    }
   ],
   "source": [
    "# 定义填充函数\n",
    "def fill_missing_values(df, column, similar_column, data):\n",
    "    if df[column].isnull().any():  # 检查目标列是否有缺失值\n",
    "        correlation = data\n",
    "\n",
    "        if correlation < 0.5 or df[similar_column].isnull().any():\n",
    "            # 使用均值填充\n",
    "            print(f\"列'{column}'均值填充\")\n",
    "            mean_value = df[column].mean()\n",
    "            df[column].fillna(mean_value, inplace=True)\n",
    "        else:\n",
    "            # 使用线性回归预测填充\n",
    "            print(f\"列'{column}'线性回归预测填充\")\n",
    "            known_data = df[[column, similar_column]].dropna()\n",
    "            unknown_data = df[df[column].isnull()]\n",
    "            \n",
    "            # 定义特征和目标列\n",
    "            features = [similar_column]\n",
    "            target_columns = [column]\n",
    "\n",
    "            # 划分训练集和测试集\n",
    "            X_train  = known_data[features] \n",
    "            y_train=known_data[target_columns]\n",
    "            # 训练线性回归模型\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # 预测缺失值\n",
    "            predicted_values = model.predict(unknown_data[features])\n",
    "\n",
    "            # 填充缺失值\n",
    "            unknown_data[target_columns] = predicted_values\n",
    "            filled_data = known_data._append(unknown_data)\n",
    "            \n",
    "            df.loc[filled_data.index, [column, similar_column]] = filled_data[[column, similar_column]]\n",
    "\n",
    "# 遍历每个字段\n",
    "for key, (first_element, second_element) in similarities.items():\n",
    "    fill_missing_values(Corr_data, key, first_element, second_element)\n",
    "# 检查是否还有缺失值\n",
    "missing_values = Corr_data.isnull().sum().sum()\n",
    "if missing_values == 0:\n",
    "    print(\"数据中不存在任何缺失值。\")\n",
    "else:\n",
    "    print(f\"数据中仍有缺失值。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均值和线性回归结合填充 : 315.23808823529413\n",
      "均值填充 : 2506.432271237877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 提取 'zujin' 列作为标签\n",
    "labels = ground_data50[\"zujin\"]\n",
    "# 定义决策树回归器\n",
    "tree_regressor = DecisionTreeRegressor()\n",
    "\n",
    "mae_mean_cv_line = cross_val_score(tree_regressor, Corr_data, labels, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv_line = -mae_mean_cv_line.mean()  # 取负号得到正的 MAE\n",
    "\n",
    "mae_mean_cv50 = cross_val_score(tree_regressor, ground_data50, labels, cv=10, scoring='neg_mean_absolute_error')\n",
    "mae_mean_cv50 = -mae_mean_cv50.mean()  # 取负号得到正的 MAE\n",
    "print(\"均值和线性回归结合填充 :\", mae_mean_cv_line)\n",
    "print(\"均值填充 :\", mae_mean_cv50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果显示采用线性回归和均值填充结合的方法效果更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列'chengbaogd'均值填充\n",
      "列'jygdarea'均值填充\n",
      "列'jycbgdarea'均值填充\n",
      "列'jyczgdarea'均值填充\n",
      "列'jynhgdarea'线性回归预测填充\n",
      "列'zcgdarea'均值填充\n",
      "列'zcqiyearea'均值填充\n",
      "列'zchezuoshearea'均值填充\n",
      "列'zccunjitiarea'线性回归预测填充\n",
      "列'zcnonghuarea'线性回归预测填充\n",
      "列'zcareahetong'线性回归预测填充\n",
      "列'zcareaqixian'线性回归预测填充\n",
      "列'jitizcarea'均值填充\n",
      "列'jitizcareacgb'线性回归预测填充\n",
      "列'zujin'均值填充\n",
      "列'cbdarea'均值填充\n",
      "列'zrdarea'均值填充\n",
      "列'cbdcwjuli'均值填充\n",
      "列'zrdcwjuli'线性回归预测填充\n",
      "列'cbddljuli'均值填充\n",
      "列'zrddljuli'均值填充\n",
      "列'zrdyear'均值填充\n",
      "列'zrdsyqixian'线性回归预测填充\n",
      "列'zrdzuyue'均值填充\n",
      "列'd302a'均值填充\n",
      "列'd302b'均值填充\n",
      "列'd303a'均值填充\n",
      "列'd303b'线性回归预测填充\n",
      "列'd306a'均值填充\n",
      "列'd306b'线性回归预测填充\n",
      "列'd307a'均值填充\n",
      "列'd307b'线性回归预测填充\n",
      "列'd308a'均值填充\n",
      "列'd308b'线性回归预测填充\n",
      "列'd309b'线性回归预测填充\n",
      "列'd318a'均值填充\n",
      "列'd318b'均值填充\n",
      "列'd327a'均值填充\n",
      "列'd327b'线性回归预测填充\n",
      "列'd330a'均值填充\n",
      "列'd330b'线性回归预测填充\n",
      "列'd332a'均值填充\n",
      "列'd332b'线性回归预测填充\n",
      "列'cychengbaogd'线性回归预测填充\n",
      "存在负值，将使用填充前的数据的平均值来替换。\n",
      "数据中不存在任何缺失值。\n"
     ]
    }
   ],
   "source": [
    "# 用线性回归结合均值填充\n",
    "ground_data = pd.read_csv(r\"C:\\Users\\12045\\Desktop\\地块数据(预处理中).csv\")\n",
    "# 删除户数据中缺失值占比超过50%的特征列，但保留 \"zujin\" 列\n",
    "missing_percentage = (ground_data.isnull().sum() / len(ground_data)) * 100\n",
    "columns_with_high_missing_percentage = missing_percentage[missing_percentage > 50].index\n",
    "columns_with_high_missing_percentage = columns_with_high_missing_percentage.drop(\"zujin\")\n",
    "ground_data50 = ground_data.drop(columns=columns_with_high_missing_percentage)\n",
    "# 对分类数据缺失值采用众数填充\n",
    "columns_to_fill = [\"cbdfangwei\", \"zrdfangwei\", \"zjxingshi\", \"c214b\", \"cbdpodu\", \"zrdpodu\", \"cbdgaosulu\",\n",
    "                   \"zrdgaosulu\", \"cbdturang\", \"zrdturang\", \"cbdguangai\", \"zrdguangai\", \"cbdfeili\", \"zrdfeili\",\n",
    "                   \"cbdyongtu201912\", \"zrdyongtu201912\", \"cbdyongtu202008\", \"zrdyongtu202008\", \"cbdwrxiufu\",\n",
    "                   \"zrdwrxiufu\", \"shifouzc\", \"cbdjinqin\", \"zrdjinqin\", \"cbdbutie\", \"zrdbutie\", \"cbdxietiao\",\n",
    "                   \"zrdxietiao\", \"cbdhetong\", \"zrdhetong\", \"cbdjypt\", \"zrdjypt\", \"cbdjyptreason1\",\n",
    "                   \"zrdjyptreason1\", \"cbdjyptreason2\", \"zrdjyptreason2\", \"cbdjiangqixian\", \"zrdjiangqixian\",\n",
    "                   \"d301a\", \"d301b\", \"d311a\", \"d311b\", \"d312a\", \"d312b\", \"d313a\", \"d313b\", \"d314a\", \"d314b\",\n",
    "                   \"d315a\", \"d315b\", \"d316a\", \"d316b\", \"d317a\", \"d317b\", \"d319a\", \"d319b\", \"d320a\", \"d320b\",\n",
    "                   \"d321a\", \"d321b\", \"d322a\", \"d322b\", \"d323a\", \"d323b\", \"d324a\", \"d324b\", \"d325a\", \"d325b\",\n",
    "                   \"d328a\", \"d328b\", \"d329a\", \"d329b\", \"d331a\", \"d331b\", \"d335a\", \"d335b\", \"d336a\", \"d336b\",\n",
    "                   \"d337a\", \"d337b\", \"d338a\", \"d338b\", \"d339a\", \"d339b\", \"d340a\", \"d340b\", \"d341a\", \"d341b\"\n",
    "                   \"cbdpodu\",\"zrdpodu\",\"cbdturang\",\"zrdturang\",\"cbdyongtu201912\",\"zrdyongtu201912\",\n",
    "                   \"cbdyongtu202008\",\"zrdyongtu202008\",\"cbdjiangqixian\",\"cbdqixian\",\"zrdqixian\",\"d301a\",\"d301b\",\n",
    "                   \"d336a\",\"d336b\",\"hid\",\"jygdnum\",\"jygdnum1\",\"jygdnum5\",\n",
    "                    \"zcgdksnum\",\"zcgdksnum\",\"chengbaogdks\"]\n",
    "\n",
    "# 填充缺失值\n",
    "for column in ground_data50:\n",
    "    if column in columns_to_fill:\n",
    "        mode_val = ground_data50[column].mode()[0]  # 计算众数并取第一个值\n",
    "        ground_data50[column].fillna(mode_val, inplace=True)\n",
    "# 复制数据集\n",
    "Corr_data = ground_data50.copy()\n",
    "# 各字段间相关程度\n",
    "correlation = Corr_data.corr()\n",
    "# 创建一个空字典来保存每个字段与其相似性最高的另一字段名称及相似性\n",
    "similarities = {}\n",
    "# 遍历数据框的列\n",
    "for column in Corr_data.columns:\n",
    "    # 计算该列与其他列的相关性\n",
    "    correlations = Corr_data.corr()[column].drop(column)\n",
    "    # 找到与当前列相关性最高的字段名和相似性值\n",
    "    most_similar_column = correlations.idxmax()\n",
    "    highest_correlation = correlations.max()\n",
    "    # 保存到字典中\n",
    "    similarities[column] = (most_similar_column, highest_correlation)\n",
    "def fill_missing_values(df, column, similar_column, data):\n",
    "    if df[column].isnull().any():  # 检查目标列是否有缺失值\n",
    "        correlation = data\n",
    "\n",
    "        if correlation < 0.5 or df[similar_column].isnull().any():\n",
    "            # 使用均值填充\n",
    "            print(f\"列'{column}'均值填充\")\n",
    "            mean_value = df[column].mean()\n",
    "            df[column].fillna(mean_value, inplace=True)\n",
    "        else:\n",
    "            # 使用线性回归预测填充\n",
    "            print(f\"列'{column}'线性回归预测填充\")\n",
    "            known_data = df[[column, similar_column]].dropna()\n",
    "            unknown_data = df[df[column].isnull()]\n",
    "            \n",
    "            # 定义特征和目标列\n",
    "            features = [similar_column]\n",
    "            target_column = column\n",
    "\n",
    "            # 划分训练集和测试集\n",
    "            X_train = known_data[[similar_column]]\n",
    "            y_train = known_data[column]\n",
    "            \n",
    "            # 训练线性回归模型\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # 预测缺失值\n",
    "            predicted_values = model.predict(unknown_data[[similar_column]])\n",
    "\n",
    "            # 填充缺失值\n",
    "            df.loc[df[column].isnull(), column] = predicted_values\n",
    "\n",
    "# 遍历每个字段\n",
    "for key, (first_element, second_element) in similarities.items():\n",
    "    fill_missing_values(Corr_data, key, first_element, second_element)\n",
    "    \n",
    "# 填充缺失值后检查是否存在负值\n",
    "negative_values = (Corr_data < 0).any().any()\n",
    "if negative_values:\n",
    "    print(\"存在负值，将使用填充前的数据的平均值来替换。\")\n",
    "    # 填充前的数据\n",
    "    original_data = ground_data50\n",
    "\n",
    "    for column in Corr_data.columns:\n",
    "        # 只对存在负值的列进行处理\n",
    "        negative_mask = Corr_data[column] < 0\n",
    "        if negative_mask.any():\n",
    "            # 使用填充前的数据的平均值替换负值\n",
    "            mean_value = original_data[column].mean()\n",
    "            Corr_data.loc[negative_mask, column] = mean_value\n",
    "# 检查是否还有缺失值\n",
    "missing_values = Corr_data.isnull().sum().sum()\n",
    "if missing_values == 0:\n",
    "    print(\"数据中不存在任何缺失值。\")\n",
    "    ground_data=Corr_data\n",
    "else:\n",
    "    print(f\"数据中仍有缺失值。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_data.to_csv(r\"C:\\Users\\12045\\Desktop\\地块数据(无标准化用于可视化).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_to_normalize = [col for col in ground_data.columns if col not in [\"cbdfangwei\", \"zrdfangwei\", \"zjxingshi\", \"c214b\", \"cbdpodu\", \"zrdpodu\", \"cbdgaosulu\",\n",
    "                   \"zrdgaosulu\", \"cbdturang\", \"zrdturang\", \"cbdguangai\", \"zrdguangai\", \"cbdfeili\", \"zrdfeili\",\n",
    "                   \"cbdyongtu201912\", \"zrdyongtu201912\", \"cbdyongtu202008\", \"zrdyongtu202008\", \"cbdwrxiufu\",\n",
    "                   \"zrdwrxiufu\", \"shifouzc\", \"cbdjinqin\", \"zrdjinqin\", \"cbdbutie\", \"zrdbutie\", \"cbdxietiao\",\n",
    "                   \"zrdxietiao\", \"cbdhetong\", \"zrdhetong\", \"cbdjypt\", \"zrdjypt\", \"cbdjyptreason1\",\n",
    "                   \"zrdjyptreason1\", \"cbdjyptreason2\", \"zrdjyptreason2\", \"cbdjiangqixian\", \"zrdjiangqixian\",\n",
    "                   \"d301a\", \"d301b\", \"d311a\", \"d311b\", \"d312a\", \"d312b\", \"d313a\", \"d313b\", \"d314a\", \"d314b\",\n",
    "                   \"d315a\", \"d315b\", \"d316a\", \"d316b\", \"d317a\", \"d317b\", \"d319a\", \"d319b\", \"d320a\", \"d320b\",\n",
    "                   \"d321a\", \"d321b\", \"d322a\", \"d322b\", \"d323a\", \"d323b\", \"d324a\", \"d324b\", \"d325a\", \"d325b\",\n",
    "                   \"d328a\", \"d328b\", \"d329a\", \"d329b\", \"d331a\", \"d331b\", \"d335a\", \"d335b\", \"d336a\", \"d336b\",\n",
    "                   \"d337a\", \"d337b\", \"d338a\", \"d338b\", \"d339a\", \"d339b\", \"d340a\", \"d340b\", \"d341a\", \"d341b\"\n",
    "                   \"cbdpodu\",\"zrdpodu\",\"cbdturang\",\"zrdturang\",\"cbdyongtu201912\",\"zrdyongtu201912\",\n",
    "                   \"cbdyongtu202008\",\"zrdyongtu202008\",\"cbdjiangqixian\",\"cbdqixian\",\"zrdqixian\",\"d301a\",\n",
    "                   \"d301b\",\"d336a\",\"d336b\",\"hid\",\"jygdnum\",\"jygdnum1\",\"jygdnum5\",\n",
    "                    \"zcgdksnum\",\"zcgdksnum\",\"chengbaogdks\"]]\n",
    "\n",
    "# 对这些列进行标准化处理\n",
    "scaler = StandardScaler()\n",
    "ground_data[columns_to_normalize] = scaler.fit_transform(ground_data[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_data.to_csv(r\"C:\\Users\\12045\\Desktop\\地块数据(预处理后).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4处理村数据，由于村数据缺失值较少，采用任意填充方法区别不大，故使用最简单的均值填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 使用均值填充缺失值\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "village_data_mean = mean_imputer.fit_transform(village_data)\n",
    "village_data_mean_df = pd.DataFrame(village_data_mean, columns=village_data.columns.tolist())\n",
    "\n",
    "# 将数据保存到 CSV 文件中（未标准化）\n",
    "village_data_mean_df.to_csv(r\"C:\\Users\\12045\\Desktop\\村数据(无标准化用于可视化).csv\", index=False)\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler()\n",
    "village_data_scaled = scaler.fit_transform(village_data_mean)\n",
    "\n",
    "# 将标准化后的数据转换为 DataFrame\n",
    "village_data_scaled_df = pd.DataFrame(village_data_scaled, columns=village_data.columns.tolist())\n",
    "\n",
    "# 输出预处理后的数据到 CSV 文件中\n",
    "village_data_scaled_df.to_csv(r\"C:\\Users\\12045\\Desktop\\村数据(预处理后).csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
